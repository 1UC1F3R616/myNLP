{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "NLPcourse1.5Hours.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyM0nde9biAsQGqg0pEicXpk",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/gist/1UC1F3R616/87126d413efcb6d4097f48bfb5f77b9d/nlpcourse1-5hours.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wh4n-bnGgcIb",
        "colab_type": "text"
      },
      "source": [
        "## Tokenize\n",
        "- word\n",
        "- sentence\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hJkjpRavPd2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Riw3L1IUwVq1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset = 'Hi My dataset is created by Me. Lol I know'\n",
        "dataset2 = \"\"\"Hello Mr. Watson, how are you doing today?\n",
        "             The weather is awsome. The garden is green.\n",
        "             We should go out for a walk.\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4LYm-xvG1kRl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "06a7fd89-0043-4489-c299-4adbe161fa56"
      },
      "source": [
        "import nltk\n",
        "nltk.download('punkt')"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_2838bB80kSb",
        "colab_type": "code",
        "outputId": "11c39de2-4f96-4f74-f22c-daca0f7abc2d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        }
      },
      "source": [
        "print(sent_tokenize(dataset))\n",
        "print(sent_tokenize(dataset2))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Hi My dataset is created by Me.', 'Lol I know']\n",
            "['Hello Mr. Watson, how are you doing today?', 'The weather is awsome.', 'The garden is green.', 'We should go out for a walk.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cuxQy_Cq1y-o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(word_tokenize(dataset))\n",
        "print(word_tokenize(dataset2))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8QAJiRVc173W",
        "colab_type": "text"
      },
      "source": [
        "## Stop Words\n",
        "- search engines are programmed to ignore the Stop Words"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fjKejhfY2AV0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mSxUabF22Xyh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_j_7Ggwl210h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "stop_words = set(stopwords.words('english'))\n",
        "print(stop_words)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ukjRZWT3DSc",
        "colab_type": "code",
        "outputId": "d31cf3a2-6347-4a88-a63a-d325201ce9fe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "words = word_tokenize(dataset)\n",
        "filtered_words = []\n",
        "for word in word_tokenize(dataset):\n",
        "  if word not in stop_words:\n",
        "    filtered_words.append(word)\n",
        "\n",
        "print(filtered_words)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['Hi', 'My', 'dataset', 'created', 'Me', '.', 'Lol', 'I', 'know']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CxEhcnIx6jkN",
        "colab_type": "text"
      },
      "source": [
        "## Stemming -\n",
        "The process of removing prefixes, suffixes from the words and reduce them to their stem form.\n",
        "\n",
        "For example, the word \"computation\" might be stemmed to \"comput\"\n",
        "\n",
        "- Porter Stemming - The most common algorithm used for stemming in English,\n",
        "It consists of several phases of word reductions that are applied sequentially."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OxhwSHh1cXR3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "ps = PorterStemmer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O79MKwDFd5D3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dataset3 = ['love', 'lover', 'loving', 'loved', 'lovingly']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IbvL481HeDdS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for w in dataset3:\n",
        "  print(ps.stem(w))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yEZkaw9ZeZph",
        "colab_type": "text"
      },
      "source": [
        "## Tagging\n",
        "- Automatic assignment of descriptors to the given tokens is called Tagging\n",
        "- Tagging is a kind of classification\n",
        "\n",
        "### POS (Parts-Of-Speech) Tagging\n",
        "- The process of assigning one of the parts of a speech to the given word is POS tagging [noun, pronoun, verb, adverb, preposition, conjective, adjective, interjection] \n",
        "- eg. word:paper, tag:noun\n",
        "\n",
        "- POS tagger is a program that does the job of POS tagging\n",
        "- Taggers use several kinds of information: dictionaries, lexicons, rules\n",
        "There are mainly two types of taggers rule-based and stochastic\n",
        "  - Rule-bases tagger:\n",
        "     - Hand-writtem rules to distinguish\n",
        "  - Stochastic taggers\n",
        "    - HMM based\n",
        "    - likelihood of the word, tag sequence probability\n",
        "    - decision trees and maximum entropy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B_mxZ1lwgRpG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vtTwez2khVrP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nltk.download('averaged_perceptron_tagger')\n",
        "nltk.download('tagsets')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yXzUYGDXhEuc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        },
        "outputId": "61a0eb24-a768-45d1-cd59-49a4e8e7ada7"
      },
      "source": [
        "pos_tag(word_tokenize(dataset))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Hi', 'NNP'),\n",
              " ('My', 'NNP'),\n",
              " ('dataset', 'NN'),\n",
              " ('is', 'VBZ'),\n",
              " ('created', 'VBN'),\n",
              " ('by', 'IN'),\n",
              " ('Me', 'NNP'),\n",
              " ('.', '.'),\n",
              " ('Lol', 'NNP'),\n",
              " ('I', 'PRP'),\n",
              " ('know', 'VBP')]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dc4MhC7ehcZy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nltk.help.upenn_tagset()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUtZ780HiYJG",
        "colab_type": "text"
      },
      "source": [
        "## Chunking\n",
        "- like we don't memorise a phone nnumber as seprate individual numbers, we group them together to memorise easily\n",
        "- chunking is grouping of information"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mo3UvOQki2tV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.chunk import RegexpParser"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pRIeBg-8j_Qm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tokenized_data = word_tokenize(dataset)\n",
        "pos_tagging = pos_tag(tokenized_data)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ia6EYlVXkOTo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "chunk_sequence = \"\"\"\n",
        "chunk:\n",
        "{<NNPS>+}\n",
        "{<NNP>+}\n",
        "{<NN>}\"\"\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AdoKGb84kaEw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "chunk = RegexpParser(chunk_sequence)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o1hUnQQokeZB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 220
        },
        "outputId": "1ec8d877-0c7f-4596-fdd6-2268f45e09c3"
      },
      "source": [
        "chunked_data = chunk.parse(pos_tagging)\n",
        "print(chunked_data)"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(S\n",
            "  (chunk Hi/NNP My/NNP)\n",
            "  (chunk dataset/NN)\n",
            "  is/VBZ\n",
            "  created/VBN\n",
            "  by/IN\n",
            "  (chunk Me/NNP)\n",
            "  ./.\n",
            "  (chunk Lol/NNP)\n",
            "  I/PRP\n",
            "  know/VBP)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ryaQ8OPjkxnQ",
        "colab_type": "text"
      },
      "source": [
        "## Named Entity Recognition\n",
        "- Also known as\n",
        "  - Entity Identification\n",
        "  - Entity Chunking\n",
        "  - Entity Extraction\n",
        "- It is a subtask of information extraction that classify named entities into pre-defined categories such as names of persons, organizations, locations\n",
        "- Tesla: Organization, Elon Musk: Person\n",
        "\n",
        "### Applications\n",
        "- classify the contents to news providers\n",
        "- Efficent search Algorithms\n",
        "- Content recommendation\n",
        "- Question and Answer systems\n",
        "- Automatic Forwarding\n",
        "- Online document searching"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kkH-Kq3GmWeF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "from nltk.chunk import ne_chunk"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DvXxnkaMmmTh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "tagged_data = pos_tag(word_tokenize(dataset))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HdQrnU5UnA_V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nltk.download('maxent_ne_chunker')\n",
        "nltk.download('words')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tJ6AOwymr8U",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 239
        },
        "outputId": "57a9ff2e-d4ba-4883-ba88-9c84fbb7a8a5"
      },
      "source": [
        "# Applying Named Entity Recognization with ne_chunk\n",
        "ne_data = ne_chunk(tagged_data)\n",
        "print(ne_data)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(S\n",
            "  Hi/NNP\n",
            "  My/NNP\n",
            "  dataset/NN\n",
            "  is/VBZ\n",
            "  created/VBN\n",
            "  by/IN\n",
            "  Me/NNP\n",
            "  ./.\n",
            "  Lol/NNP\n",
            "  I/PRP\n",
            "  know/VBP)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lPOdMvJXnQ-c",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# won't work on GC\n",
        "# ne_data.draw()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y2WPDqnhpR5y",
        "colab_type": "text"
      },
      "source": [
        "## Lemmatization\n",
        "- Process of converting the words of a sentence into it's dictionary form.\n",
        "word: Feet, Lemma: Foot\n",
        "  - we get a meaningful name\n",
        "  \n",
        "## Stemming\n",
        "- Process of converting the words of a sentence to it's non-changing portions\n",
        "  - we may or may not get a meaningful name"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MCxK0EipzN7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.stem import WordNetLemmatizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GkWCH-oDrDSX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "wnl = WordNetLemmatizer()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FIBAA8nvrgfd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "2cb8c029-74de-4904-894d-41a464fea654"
      },
      "source": [
        "nltk.download('wordnet')"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/wordnet.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mur4H0pLrHWY",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        },
        "outputId": "43597842-79ba-462e-9560-e711437f2406"
      },
      "source": [
        "words = ['dogs', 'cars', 'feet', 'people']\n",
        "for word in words:\n",
        "  print(wnl.lemmatize(word))\n",
        "\n",
        "print(wnl.lemmatize('fantasized', 'v'))"
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dog\n",
            "car\n",
            "foot\n",
            "people\n",
            "fantasize\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7ZvFDRzrmCZ",
        "colab_type": "text"
      },
      "source": [
        "## Corpus\n",
        "- Large collection of text\n",
        "- spoken material on which liguistic analysis is based"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9ybSUJrhrvui",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        },
        "outputId": "da0f29af-75ac-4765-9529-1d0a1d57a9e0"
      },
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('state_union')\n",
        "from nltk.corpus import state_union\n",
        "\n",
        "dataset = state_union.raw('2001-GWBush-1.txt')"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]   Package state_union is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jebmANCstb3N",
        "colab_type": "text"
      },
      "source": [
        "## Wordnet\n",
        "- Lexical database in english language\n",
        "- It group english words in antonyms and synonyms\n",
        "- Also provides short examples and words\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QB_ru1yMtsnY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from nltk.corpus import wordnet"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLUlC5qLtwAH",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 55
        },
        "outputId": "f1bbf934-c46a-47e3-d83a-b8151227eb64"
      },
      "source": [
        "syns = wordnet.synsets('program')\n",
        "print(syns)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Synset('plan.n.01'), Synset('program.n.02'), Synset('broadcast.n.02'), Synset('platform.n.02'), Synset('program.n.05'), Synset('course_of_study.n.01'), Synset('program.n.07'), Synset('program.n.08'), Synset('program.v.01'), Synset('program.v.02')]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NHoI-m_Et7SP",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "2a8a6132-2761-47c6-821b-74554fe70356"
      },
      "source": [
        "print(syns[0].lemmas())\n",
        "print(syns[0].lemmas()[0].name()) # Getting the name of it\n",
        "print(syns[0].lemmas()[1].name())"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Lemma('plan.n.01.plan'), Lemma('plan.n.01.program'), Lemma('plan.n.01.programme')]\n",
            "plan\n",
            "program\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6c6N5MbFuNj5",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "c3fa936e-4a40-41d3-bbf3-2a7d5025fb98"
      },
      "source": [
        "# Getting the meaning of that word\n",
        "print(syns[0].definition())"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "a series of steps to be carried out or goals to be accomplished\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qv6KsSoUunDy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "8ca4f998-da6a-4277-994a-e7e52cf3d78e"
      },
      "source": [
        "print(syns[0].examples())"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['they drew up a six-step plan', 'they discussed plans for a new bond issue']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PlFj8gbHuvj2",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "outputId": "e670aa91-6f04-47be-e266-fd3f00bde101"
      },
      "source": [
        "# Antonyms\n",
        "antonyms = []\n",
        "synonyms = []\n",
        "\n",
        "for syn in wordnet.synsets('good'):\n",
        "  for l in syn.lemmas():\n",
        "    synonyms.append(l.name())\n",
        "    if l.antonyms():\n",
        "      antonyms.append(l.antonyms()[0].name())\n",
        "\n",
        "print(antonyms)\n",
        "print(synonyms)"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['evil', 'evilness', 'bad', 'badness', 'bad', 'evil', 'ill']\n",
            "['good', 'good', 'goodness', 'good', 'goodness', 'commodity', 'trade_good', 'good', 'good', 'full', 'good', 'good', 'estimable', 'good', 'honorable', 'respectable', 'beneficial', 'good', 'good', 'good', 'just', 'upright', 'adept', 'expert', 'good', 'practiced', 'proficient', 'skillful', 'skilful', 'good', 'dear', 'good', 'near', 'dependable', 'good', 'safe', 'secure', 'good', 'right', 'ripe', 'good', 'well', 'effective', 'good', 'in_effect', 'in_force', 'good', 'good', 'serious', 'good', 'sound', 'good', 'salutary', 'good', 'honest', 'good', 'undecomposed', 'unspoiled', 'unspoilt', 'good', 'well', 'good', 'thoroughly', 'soundly', 'good']\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}